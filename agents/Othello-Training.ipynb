{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23dc9e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import copy\n",
    "import pstats\n",
    "\n",
    "from collections import deque\n",
    "import gym\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "# for performance profiling\n",
    "import cProfile as cprofile\n",
    "from memory_profiler import profile\n",
    "fp = open(\"report-trn.log\", \"w+\")  # to capture memory profile logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5cea944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf17926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# setting this to ensure that we can reproduce the results\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# setting log level for tensorflow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# setting OS variables for Tensorflow\n",
    "os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'\n",
    "os.environ['TF_GPU_THREAD_COUNT'] = '4' #if not hvd_utils.is_using_hvd() else str(hvd.size())\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Keras RL\n",
    "from rl.agents.dqn import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebccc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_gpu(gpu_ids_list):\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            gpus_used = [gpus[i] for i in gpu_ids_list]\n",
    "            tf.config.set_visible_devices(gpus_used, 'GPU')\n",
    "            for gpu in gpus_used:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "        except RuntimeError as e:\n",
    "            # Visible devices must be set before GPUs have been initialized\n",
    "            print(e)\n",
    "\n",
    "\n",
    "set_gpu([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"othello:othello-v0\"\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "env.observation_space['state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd729ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of observations\n",
    "num_observations = env.observation_space['state'].shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print(num_observations, num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17087cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import othello\n",
    "from othello import othello_agent\n",
    "\n",
    "import importlib\n",
    "import sys\n",
    "importlib.reload(sys.modules.get('othello.othello_agent'))\n",
    "\n",
    "\n",
    "agent_white = othello_agent.OthelloDQN(nb_observations=64, player=\"white\")\n",
    "# agent_white.model_target.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f646c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20000\n",
    "\n",
    "is_white = []\n",
    "reward_history = []\n",
    "winning_rate = []\n",
    "best_winning_rate = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    ep_reward = []\n",
    "    \n",
    "    observation, info = env.reset()\n",
    "    observation = observation[\"state\"].reshape((1,64))\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        next_possible_actions = info[\"next_possible_actions\"]\n",
    "        \n",
    "        if info[\"next_player\"][\"name\"] == \"white\":\n",
    "            action = agent_white.choose_action(observation, next_possible_actions)\n",
    "\n",
    "            next_observation, reward, done, truncated, info = env.step(action)\n",
    "            next_observation = next_observation[\"state\"].reshape((1,64))\n",
    "\n",
    "            agent_white.store_transition(observation, action, reward, done, next_observation)\n",
    "            \n",
    "            if done:\n",
    "                print(\"Storing transition for last move by white. \", \"Winner:\", info[\"winner\"], \"Reward:\", reward)\n",
    "\n",
    "            ep_reward.append(reward)\n",
    "        else:\n",
    "            action = random.choice(list(next_possible_actions))\n",
    "            action = (action[0] * 8) + action[1]\n",
    "\n",
    "            next_observation, reward, done, truncated, info = env.step(action)\n",
    "            next_observation = next_observation[\"state\"].reshape((1,64))\n",
    "\n",
    "            # this is to cater for the case when the last move is by the black player, we want to store the\n",
    "            # previous move by white that lead to the win/loss\n",
    "            if done:\n",
    "                if info[\"winner\"] == \"White\":\n",
    "                    print(\"Storing transition for last move by black. \", \"Winner:\", info[\"winner\"], \"Reward:\", 10)\n",
    "                    agent_white.reward_transition_update(10)\n",
    "                elif info[\"winner\"] == \"Black\":\n",
    "                    print(\"Storing transition for last move by black. \", \"Winner:\", info[\"winner\"], \"Reward:\", -10)\n",
    "                    agent_white.reward_transition_update(-10)\n",
    "                elif info[\"winner\"] == \"Tie\":\n",
    "                    print(\"Storing transition for last move by black. \", \"Winner:\", info[\"winner\"], \"Reward:\", 2)\n",
    "                    agent_white.reward_transition_update(2)\n",
    "                    \n",
    "        observation = copy.deepcopy(next_observation)\n",
    "        \n",
    "\n",
    "    if done:\n",
    "        agent_white.learn()  # train agent after each trial\n",
    "        is_white.append(True if info[\"winner\"] == \"White\" else False)\n",
    "\n",
    "    # this is reward_history for white\n",
    "    reward_history.append(np.sum(ep_reward))\n",
    "    \n",
    "    \n",
    "    if (epoch % 30 == 0) and (epoch > 1):  # log winning rate in every 30 eps\n",
    "        winning_rate.append((epoch, np.mean(is_white)))\n",
    "        is_white = []\n",
    "        print(\"\\n***** Epoch: {:d}/{:d}, white player winning rate in latest 30 rounds: {:.2%}. *****\\n\".format(epoch, EPOCHS, winning_rate[-1][1]))\n",
    "        \n",
    "        if (winning_rate[-1][1] >= best_winning_rate):\n",
    "            agent_white.save_model(name=\"OthelloDQN\")\n",
    "            print(\"\\n***** Save model at Epoch: {:d}/{:d}\\n\".format(epoch, EPOCHS))\n",
    "            best_winning_rate = winning_rate[-1][1]\n",
    "\n",
    "        # memory cleanup\n",
    "        n = gc.collect()\n",
    "        print(\"\\nNumber of unreachable objects collected by GC:\", n, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55400f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "251eb2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "curr_date = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "path = \"./models/{:s}/\".format(curr_date)\n",
    "\n",
    "# IF no such folder exists, create one automatically\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "# open a binary file in write mode\n",
    "file = open(path + \"winning_rate_{:s}\".format(curr_date), \"wb\")\n",
    "# save array to the file\n",
    "np.save(file, winning_rate)\n",
    "# close the file\n",
    "file.close\n",
    "\n",
    "# open the file in read binary mode\n",
    "file = open(path + \"winning_rate_{:s}\".format(curr_date), \"rb\")\n",
    "#read the file to numpy array\n",
    "winning_rate = np.load(file)\n",
    "#close the file\n",
    "print(winning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average = sum(win_rate for epoch, win_rate in winning_rate) / len(winning_rate)\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3011b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
